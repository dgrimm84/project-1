# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time
from pprint import pprint
import json
from scipy.stats import linregress
import scipy.stats as st
import csv
import os
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import seaborn as sns
import geopandas as gpd
import folium
import hvplot.pandas
from folium.plugins import HeatMap
from folium.plugins import MarkerCluster
file_to_output = os.path.join("project-1", "output.csv")


#url ="https://www.fema.gov/api/open/v2/DisasterDeclarationsSummaries"
#url= "https://www.fema.gov/api/open/v1/FemaWebDisasterSummaries"
url="https://www.fema.gov/api/open/v2/HazardMitigationGrantProgramDisasterSummaries"
#filters1 = "$filter=incidentType ne 'Severe Storm'"
filters2 = "$filter=declarationDate gt '2004-01-13T04:00:00.000Z' and (incidentType eq 'Hurricane' or incidentType eq 'Typhoon' or incidentType eq 'Fire' or incidentType eq 'Flood')" 
#filters3 = "$filter=incidentType eq 'Fire'" -481
#filters4 = "$filter=incidentType eq 'Flood'" -355
#filters5 = "$filter=(incidentType eq 'Severe Ice Storm' or incidentType eq 'Snow Storm' or incidentType eq 'Winter Storm' or incidentType eq 'Freezing' or incidentType eq 'Coastal Storm' or incidentType eq 'Tropical Storm')"-106
#filters1 = "$filter=declarationDate gt '2014-01-13T04:00:00.000Z' and incidentType eq 'Severe Storm'" -1000
#filters1 = "$filter=declarationDate gt '2014-01-13T04:00:00.000Z' and (incidentType eq 'Hurricane' or incidentType eq 'Typhoon')"



# Send a GET request to the API
#response = requests.get(url+'?'+filters1)
response = requests.get(f"{url}?{filters2}")


# Send a GET request to the API


# Check if the request was successful
if response.status_code == 200:
    # Parse the JSON data
    data = response.json()
    
    # Extract the list of disaster declarations
    #disasters = data.get('DisasterDeclarationsSummaries', [])
    #disasters = data.get('FemaWebDisasterSummaries',[])
    disasters = data.get('HazardMitigationGrantProgramDisasterSummaries',[])
    # Create a DataFrame from the list of disaster records
    Disaster_df = pd.DataFrame(disasters)
    #Disaster_df.drop(Disaster_df.columns[[8,11,12,14,15,16,17]], axis=1, inplace=True)
    #columns_to_drop = ['column1', 'column2']
    Disaster_df = Disaster_df.iloc[:, [0,1,2,3,4,5,6,8,10,13,23,24,25,26,27,28,29,31]]
    # Display the DataFrame
    Disaster_df
else:
    print(f"Error: {response.status_code}")

# Convert declarationDate to datetime
Disaster_df['declarationDate'] = pd.to_datetime(Disaster_df['declarationDate'])

# Create a new column with the desired format
Disaster_df['formattedDate'] = Disaster_df['declarationDate'].dt.strftime('%B %Y')
Disaster_df['formattedyear'] = Disaster_df['declarationDate'].dt.strftime('%Y')


Disaster_df



# Check for missing values
print(Disaster_df.isnull().sum())


# Get summary statistics
print(Disaster_df.describe(include='all'))


Disaster_df.to_csv('cleaned_disaster_data.csv', index=False)


# Load the U.S. states shapefile from a URL or local file
states = gpd.read_file("https://raw.githubusercontent.com/PublicaMundi/MappingAPI/master/data/geojson/us-states.json")

# Display the states DataFrame
print(states.head())


# Assume Disaster_df has a 'state' column and 'disaster_count' column
disaster_counts = Disaster_df['state'].value_counts().reset_index()
disaster_counts.columns = ['state', 'disaster_count']

# Merge with the states GeoDataFrame
merged = states.set_index('id').join(disaster_counts.set_index('state'))
merged





Disaster_df['incidentType'].unique()


Disaster_df['incidentType'].value_counts()


monthly_counts = Disaster_df['formattedyear'].value_counts().sort_index()

# Plotting
plt.figure(figsize=(12, 6))
monthly_counts.plot(kind='bar')
plt.title('Disaster Declarations by year')
plt.xlabel('Year')
plt.ylabel('Number of Declarations')
plt.xticks(rotation=45)
plt.show()


# Count disasters per year
yearly_counts = Disaster_df['formattedyear'].value_counts().sort_index()

# Plotting
plt.figure(figsize=(12, 6))
yearly_counts.plot(kind='line', marker='o')
plt.title('IncidentType Declarations Over Years')
plt.xlabel('Year')
plt.ylabel('Number of Declarations')
plt.grid()
plt.show()


# Assuming you have a 'disasterType' column
disaster_type_counts = Disaster_df['incidentType'].value_counts()

# Plotting
plt.figure(figsize=(8, 8))
disaster_type_counts.plot(kind='pie', autopct='%1.1f%%', startangle=90)
plt.title('Proportion of incidentType Types')
plt.ylabel('')  # Hide the y-label
plt.show()


# Pivot table for heatmap
heatmap_data = Disaster_df.pivot_table(index='formattedyear', columns='incidentType', values='declarationDate', aggfunc='count', fill_value=0)

# Plotting
plt.figure(figsize=(12, 8))
sns.heatmap(heatmap_data, annot=True, fmt='d', cmap='YlGnBu')
plt.title('Heatmap of incidentType by Year and Type')
plt.xlabel('IncidentType')
plt.ylabel('Year')
plt.show()


# Plotting the choropleth map
fig, ax = plt.subplots(1, 1, figsize=(15, 10))
merged.boundary.plot(ax=ax, linewidth=1)
merged.plot(column='disaster_count', ax=ax, legend=True,
            legend_kwds={'label': "Number of Disasters by State",
                         'orientation': "horizontal"},
            cmap='OrRd')

plt.title('Disaster Declarations by State')
plt.show()






