# Dependencies and Setup
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import requests
import time
from pprint import pprint
import json
from scipy.stats import linregress
import scipy.stats as st
import csv
import os
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import seaborn as sns
from datetime import datetime

file_to_output = os.path.join("project-1", "output.csv")


#URL Bases and Filters
url = "https://www.fema.gov/api/open/v2/HazardMitigationGrantProgramDisasterSummaries"
urlv1 = "https://www.fema.gov/api/open/v1/FemaWebDisasterSummaries"
urlv2 ="https://www.fema.gov/api/open/v2/DisasterDeclarationsSummaries"


filters1 = "$filter=declarationDate gt '2003-12-31T04:00:00.000z' and incidentType eq 'Severe Storm'"
filters2 = "$filter=declarationDate gt '2003-12-31T04:00:00.000z' and (incidentType eq 'Hurricane' or incidentType eq 'Typhoon')" 
filters3 = "$filter=declarationDate gt '2003-12-31T04:00:00.000z' and incidentType eq 'Fire'" 
filters4 = "$filter=declarationDate gt '2003-12-31T04:00:00.000z' and incidentType eq 'Flood'" 
filters5 = "$filter=declarationDate gt '2003-12-31T04:00:00.000z' and (incidentType eq 'Severe Ice Storm' or incidentType eq 'Snow Storm' or incidentType eq 'Winter Storm' or incidentType eq 'Freezing' or incidentType eq 'Coastal Storm' or incidentType eq 'Tropical Storm')"



# Send a GET request to the API
response1 = requests.get(f"{url}?{filters1}")
response1


# Check if the request was successful
if response1.status_code == 200:
    # Parse the JSON data
    data = response1.json()
    
    # Extract the list of disaster declarations
    #disasters = data.get('DisasterDeclarationsSummaries', [])
    #disasters = data.get('FemaWebDisasterSummaries',[])
    disasters = data.get('HazardMitigationGrantProgramDisasterSummaries',[])
    
    # Create a DataFrame from the list of disaster records
    severe_df = pd.DataFrame(disasters)
    severe_df = severe_df.iloc[:, [0,1,2,3,4,5,6,8,10,13,23,24,25,26,27,28,29,31]]
    # Display the DataFrame
    severe_df
else:
    print(f"Error: {response1.status_code}")

# Convert declarationDate to datetime
severe_df['declarationDate'] = pd.to_datetime(severe_df['declarationDate'])

# Create a new column with the desired format
severe_df['Year'] = severe_df['declarationDate'].dt.strftime('%Y')


severe_df


severe_df["title"].value_counts()


# Send a GET request to the API
response2 = requests.get(f"{url}?{filters5}")
response2


# Check if the request was successful
if response2.status_code == 200:
    # Parse the JSON data
    data = response2.json()
    
    # Extract the list of disaster declarations
    #disasters = data.get('DisasterDeclarationsSummaries', [])
    #disasters = data.get('FemaWebDisasterSummaries',[])
    disasters = data.get('HazardMitigationGrantProgramDisasterSummaries',[])
    
    # Create a DataFrame from the list of disaster records
    storm_df = pd.DataFrame(disasters)
    storm_df = storm_df.iloc[:, [0,1,2,3,4,5,6,8,10,13,23,24,25,26,27,28,29,31]]
    # Display the DataFrame
    severe_df
else:
    print(f"Error: {response2.status_code}")

# Convert declarationDate to datetime
storm_df['declarationDate'] = pd.to_datetime(storm_df['declarationDate'])

# Create a new column with the desired format
storm_df['Year'] = storm_df['declarationDate'].dt.strftime('%Y')


storm_df


total_storm_df = pd.concat([severe_df, storm_df], ignore_index=True)
total_storm_df =total_storm_df.sort_values(by=['Year','obligatedRegularAmount'],ascending=True)
total_storm_df.drop_duplicates(subset = ["disasterNumber"], keep = "first")
total_storm_df


### SEVERE STORM + Other STORMS FREQUENCY ###
# 20-year range List
years = []
for x in range(4,25):
    year = 2000 + x
    years.append(year)

frequency = total_storm_df["Year"].value_counts().sort_index()
ax = frequency.plot(kind='bar', color= "navy")
plt.xticks(range(len(years)), rotation = 45)
plt.xlabel("Year")
plt.ylabel("Number of Incidents")
plt.title("Disaster Declarations in the USA 2004-2024")

plt.savefig('../project-1/output/storm_frequency.png', dpi=300)
plt.show()



# Pivot table for heatmap
heatmap_data1 = total_storm_df.pivot_table(index='Year', columns='incidentType', values='declarationDate', aggfunc='count', fill_value=0)

# Plotting
plt.figure(figsize=(12, 8))
sns.heatmap(heatmap_data1, annot=True, fmt='d', cmap='YlGnBu')
plt.title('Heatmap of Incidents by Year and Type')
plt.xlabel('IncidentType')
plt.ylabel('Year')
plt.show()


merged = states_gdf.merge(total_storm_df, left_on='state_column', right_on='state_column', how='left')

# Step 3: Create the map
m = folium.Map(location=[37.8, -96], zoom_start=4)

# Create the choropleth layer
folium.Choropleth(
    geo_data=merged,
    data=merged,
    columns=['state_column', 'disaster_count'],  # Adjust these column names as necessary
    key_on='feature.properties.state_column',  # Adjust based on your GeoDataFrame's properties
    fill_color='YlOrRd',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Number of Disasters'
).add_to(m)


# Plotting the choropleth map
fig, ax = plt.subplots(1, 1, figsize=(15, 10))
total_storm_df.boundary.plot(ax=ax, linewidth=1)
total_storm_df.plot(column='disaster_count', ax=ax, legend=True,
            legend_kwds={'label': "Number of Disasters by State",
                         'orientation': "horizontal"},
            cmap='OrRd')

plt.title('Disaster Declarations by State')
plt.show()


#total_storm_df['obligatedRecipientMgmtAmt'] = total_storm_df['obligatedRecipientMgmtAmt'].map('${:,.2f}'.format)
#total_storm_df
grant_data = total_storm_df[["Year",'obligatedRecipientMgmtAmt']].groupby("Year").agg(['mean','sum','max','min'])
grant_data


ax = grant_data.plot(kind='line', color= "navy")
plt.show()



